# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1foAuXuHVpbUwGnPM7mom_MyazHTmjaYy
"""

import transformers
import torch

torch.cuda.is_available()

# PPO Training for Qwen - FIXED for Colab T4 GPU
# This version explicitly uses GPU and monitors memory usage

# First, let's check GPU availability and setup
import torch
import gc
import os

# Force CUDA if available, exit if not
if not torch.cuda.is_available():
    print("‚ùå No GPU detected! This code requires GPU.")
    exit()

print(f"‚úÖ GPU detected: {torch.cuda.get_device_name()}")
print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

# Clear any existing GPU memory
torch.cuda.empty_cache()
gc.collect()

# Set environment variables for memory efficiency
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"

# Install required packages (run this first)
# !pip install transformers==4.36.0 trl==0.7.4 peft==0.6.2 datasets bitsandbytes accelerate torch evaluate

import numpy as np
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    set_seed
)
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
import re
from torch.nn import functional as F

# Set seeds for reproducibility
set_seed(42)

# =============================================================================
# MEMORY-SAFE CONFIGURATION FOR T4
# =============================================================================

class T4Config:
    # Model settings - using smaller model for T4
    MODEL_NAME = "microsoft/DialoGPT-small"  # Much smaller, proven to work on T4
    MAX_LENGTH = 256  # Reduced context length

    # Aggressive LoRA settings
    LORA_R = 8  # Smaller rank
    LORA_ALPHA = 16
    LORA_DROPOUT = 0.1
    TARGET_MODULES = ["c_attn"]  # DialoGPT specific

    # Very conservative PPO settings for T4
    BATCH_SIZE = 1
    MINI_BATCH_SIZE = 1
    GRADIENT_ACCUMULATION_STEPS = 2  # Reduced
    LEARNING_RATE = 5e-6  # Lower LR
    PPO_EPOCHS = 2  # Fewer epochs per update

    # Training limits
    MAX_TRAIN_SAMPLES = 50  # Very small for testing
    NUM_STEPS = 20  # Just 20 steps to test

config = T4Config()

# =============================================================================
# MEMORY MONITORING UTILITIES
# =============================================================================

def print_gpu_memory():
    """Print current GPU memory usage"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        reserved = torch.cuda.memory_reserved() / 1e9
        print(f"üîã GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB")

def clear_memory():
    """Clear GPU memory"""
    gc.collect()
    torch.cuda.empty_cache()

# =============================================================================
# LIGHTWEIGHT MODEL SETUP
# =============================================================================

def setup_lightweight_model():
    """Setup the smallest possible model for T4 testing"""

    print("üîß Setting up lightweight model...")

    # Very aggressive quantization
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print_gpu_memory()

    # Load model with explicit GPU placement
    model = AutoModelForCausalLM.from_pretrained(
        config.MODEL_NAME,
        quantization_config=bnb_config,
        device_map="auto",  # This should put it on GPU
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
    )

    print(f"üìç Model device: {next(model.parameters()).device}")
    print_gpu_memory()

    # Prepare for training
    model = prepare_model_for_kbit_training(model)

    # Minimal LoRA config
    lora_config = LoraConfig(
        r=config.LORA_R,
        lora_alpha=config.LORA_ALPHA,
        target_modules=config.TARGET_MODULES,
        lora_dropout=config.LORA_DROPOUT,
        bias="none",
        task_type="CAUSAL_LM",
    )

    model = get_peft_model(model, lora_config)
    print_gpu_memory()

    return model, tokenizer

# =============================================================================
# SIMPLE MATH DATASET
# =============================================================================

def create_simple_math_data():
    """Create tiny math dataset for testing"""

    questions = [
        "What is 2 + 3?",
        "What is 5 - 2?",
        "What is 4 * 2?",
        "What is 10 / 2?",
        "What is 6 + 4?",
    ]

    answers = ["5", "3", "8", "5", "10"]

    dataset = []
    for q, a in zip(questions, answers):
        dataset.append({
            "query": f"Question: {q}\nAnswer:",
            "answer": a
        })

    return dataset

# =============================================================================
# SIMPLE REWARD MODEL
# =============================================================================

class SimpleRewardModel:
    """Very basic reward model for testing"""

    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def compute_reward(self, query, response, ground_truth):
        """Simple exact match reward"""
        # Extract number from response
        numbers = re.findall(r'\d+', response)
        predicted = numbers[0] if numbers else ""

        # Simple reward
        if predicted.strip() == ground_truth.strip():
            return 1.0
        else:
            return -0.5

# =============================================================================
# MINIMAL PPO TRAINING
# =============================================================================

def minimal_ppo_test():
    """Minimal PPO test to verify GPU usage"""

    print("üöÄ Starting minimal PPO test...")
    clear_memory()

    # Setup model
    try:
        model, tokenizer = setup_lightweight_model()
        print("‚úÖ Model loaded successfully")
    except Exception as e:
        print(f"‚ùå Model loading failed: {e}")
        return

    # Create tiny dataset
    dataset_list = create_simple_math_data()
    print(f"üìö Created dataset with {len(dataset_list)} examples")

    # PPO config for testing (simplified for current TRL version)
    ppo_config = PPOConfig(
        batch_size=config.BATCH_SIZE,
        mini_batch_size=config.MINI_BATCH_SIZE,
        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,
    )

    # Convert model for PPO
    try:
        ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(
            model,
            torch_dtype=torch.float16,
        )
        print("‚úÖ PPO model created")
        print_gpu_memory()
    except Exception as e:
        print(f"‚ùå PPO model creation failed: {e}")
        return

    # Initialize reward model
    reward_model = SimpleRewardModel(tokenizer)

    # Test a few forward passes
    print("\nüß™ Testing model inference...")

    for i, example in enumerate(dataset_list[:3]):
        print(f"\nTest {i+1}: {example['query']}")

        # Tokenize
        inputs = tokenizer(
            example['query'],
            return_tensors="pt",
            max_length=config.MAX_LENGTH,
            truncation=True,
            padding=True
        ).to('cuda')  # Explicitly move to GPU

        print(f"Input device: {inputs.input_ids.device}")

        # Generate
        with torch.no_grad():
            outputs = ppo_model.generate(
                inputs.input_ids,
                max_new_tokens=10,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.pad_token_id,
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response[len(example['query']):].strip()

        reward = reward_model.compute_reward(example['query'], response, example['answer'])

        print(f"Generated: '{response}'")
        print(f"Expected: '{example['answer']}'")
        print(f"Reward: {reward}")
        print_gpu_memory()

    print("\n‚úÖ GPU test completed successfully!")
    print("üéØ The model is using GPU and generating responses.")
    print("üöÄ You can now scale up to full PPO training!")

# =============================================================================
# MAIN EXECUTION
# =============================================================================

if __name__ == "__main__":
    print("üî• Testing PPO setup on T4 GPU")
    print("This is a minimal test to ensure GPU usage before full training.")

    minimal_ppo_test()

    print("\n" + "="*50)
    print("If this worked, your GPU setup is correct!")
    print("You can now modify for larger datasets and longer training.")
    print("="*50)