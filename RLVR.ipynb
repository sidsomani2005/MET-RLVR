{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPijlWcfJvZ0caXByR5/lXp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sidsomani2005/MET-RLVR/blob/main/RLVR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ],
      "metadata": {
        "id": "D5tzA2oH3zm8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtfoCb1x4JGV",
        "outputId": "c846c191-30d0-4dae-a724-69ce0df82103"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\",\n",
        "    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n",
        "    load_in_4bit = True,     # 4bit uses much less memory\n",
        "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # We have full finetuning now!\n",
        "    # token = \"hf_...\",      # use one if using gated models\n",
        ")"
      ],
      "metadata": {
        "id": "JGmuG2Qu3_Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,   # We support rank stabilized LoRA\n",
        "    loftq_config = None,  # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jE3rQx1X5SPU",
        "outputId": "39fccfb9-56a9-46b0-f5ca-bdad654b638f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.8.4 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "<reasoning>\n",
        "{reasoning}\n",
        "</reasoning>\n",
        "<answer>\n",
        "{answer}\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    answer = text.split(\"<answer>\")[-1]\n",
        "    answer = answer.split(\"</answer>\")[0]\n",
        "    return answer.strip()\n",
        "\n",
        "\n",
        "def extract_hash_answer(text: str) -> str | None:\n",
        "    if \"####\" not in text:\n",
        "        return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "\n",
        "\n",
        "def get_gsm8k_questions(split=\"train\") -> Dataset:\n",
        "    data = load_dataset(\"openai/gsm8k\", \"main\")[split]\n",
        "    data = data.map(\n",
        "        lambda x: {\n",
        "            \"prompt\": [\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": x[\"question\"]},\n",
        "            ],\n",
        "            \"answer\": extract_hash_answer(x[\"answer\"]),\n",
        "        }\n",
        "    )\n",
        "    return data\n",
        "\n",
        "\n",
        "dataset = get_gsm8k_questions()"
      ],
      "metadata": {
        "id": "7gaX83qa7-_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekEQ0JAc8I_T",
        "outputId": "8cd810d4-01ee-4225-d75b-afb6bcea3cb1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
              " 'answer': '72',\n",
              " 'prompt': [{'content': '\\nRespond in the following format:\\n<reasoning>\\n...\\n</reasoning>\\n<answer>\\n...\\n</answer>\\n',\n",
              "   'role': 'system'},\n",
              "  {'content': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
              "   'role': 'user'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reward_function(prompts, completions, completion_ids, tokenizer=None, **kwargs):\n",
        "    \"\"\"\n",
        "    Handles chat message style completions, extracting 'content' field.\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    for prompt, comp in zip(prompts, completions):\n",
        "        # If comp is a list with dict(s), extract the 'content' field\n",
        "        if isinstance(comp, list) and len(comp) > 0 and isinstance(comp[0], dict) and \"content\" in comp[0]:\n",
        "            text = comp[0][\"content\"]\n",
        "        elif isinstance(comp, list) and all(isinstance(t, int) for t in comp):\n",
        "            # Token ids, decode them\n",
        "            text = tokenizer.decode(comp, skip_special_tokens=True)\n",
        "        elif isinstance(comp, str):\n",
        "            text = comp\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected completion format: {type(comp)} â€” {comp}\")\n",
        "\n",
        "        score = 0.0\n",
        "        # Example reward: presence of \"correct\"\n",
        "        if \"correct\" in text.lower():\n",
        "            score += 1.0\n",
        "        length_penalty = max(0, (len(text.split()) - 50) * 0.01)\n",
        "        score -= length_penalty\n",
        "        rewards.append(score)\n",
        "    return rewards\n"
      ],
      "metadata": {
        "id": "oyYjZrmcBE-k"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import GRPOTrainer, GRPOConfig\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    reward_funcs=[reward_function],  # You can pass multiple reward functions\n",
        "    args=GRPOConfig(\n",
        "        # RL-specific args\n",
        "        per_device_train_batch_size=1,  # 1 prompt per device per step\n",
        "        gradient_accumulation_steps=4,  # effective batch size = group_size * GA\n",
        "        learning_rate=5e-6,             # Lower LR than SFT to stabilize RL\n",
        "        max_steps=200,                  # Adjust for more training\n",
        "        num_generations=4,              # Completions per prompt (group size)\n",
        "        beta=0.05,                       # KL penalty strength to keep close to reference model\n",
        "        remove_unused_columns=False,     # Keep raw dataset columns\n",
        "        logging_steps=1,\n",
        "        save_steps=50,\n",
        "        save_total_limit=2,\n",
        "        optim=\"adamw_8bit\",              # Memory-efficient optimizer\n",
        "        weight_decay=0.01,\n",
        "        warmup_steps=5,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=3407,\n",
        "        report_to=\"none\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Ei9fyAdp_vrp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}